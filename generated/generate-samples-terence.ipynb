{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157bca2e-96e0-485a-b970-1e0ed0d81eee",
   "metadata": {},
   "source": [
    "### Install related packages\n",
    "- Visit https://pytorch.org/ to install Pytorch libraries and CUDA 12.1 depending on your OS.\n",
    "- Install the transformers library\n",
    "- Ensure to have at least 16GB of GPU RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c75406-a2d2-4d80-a8f3-422b280c5d79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd4f36-0d40-46f9-b967-03a4a0beaaad",
   "metadata": {},
   "source": [
    "### Select the model to generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db076612-382f-4013-ae14-12cdde1f89b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "# model_name = \"microsoft/phi-2\"\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798d2918-a09a-4d0e-b9c9-65aecf8113f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Terence\\miniconda3\\envs\\cs5246\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:13<00:00,  6.87s/it]\n",
      "generation_config.json: 100%|█████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 19.2kB/s]\n",
      "C:\\Users\\Terence\\miniconda3\\envs\\cs5246\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Terence\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "tokenizer_config.json: 100%|███████████████████████████████████████████████████████| 1.47k/1.47k [00:00<00:00, 191kB/s]\n",
      "tokenizer.model: 100%|██████████████████████████████████████████████████████████████| 493k/493k [00:00<00:00, 12.3MB/s]\n",
      "tokenizer.json: 100%|█████████████████████████████████████████████████████████████| 1.80M/1.80M [00:00<00:00, 3.22MB/s]\n",
      "special_tokens_map.json: 100%|██████████████████████████████████████████████████████| 72.0/72.0 [00:00<00:00, 11.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "947a3979-ee82-47dc-9ca3-d96ba8d2544b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model generation parameters, tweak around max_length and temperature for more creative outputs\n",
    "# https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "generation_parameters = {\n",
    "    \"max_length\": 1024,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 5,\n",
    "    \"top_p\": 0.95,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"do_sample\": True,\n",
    "    # \"eos_token_id\": tokenizer.eos_token_id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c06ade3d-81a2-46a6-a0ff-1f3eafd6e4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_words = 512 # no of words to generate\n",
    "topics = ['politics']  # , 'riots']\n",
    "topics = ' or '.join(topics)\n",
    "prompt = f'''\n",
    "Generate some article about {topics} in around {no_words} words.\n",
    "'''\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119df568-fbab-4ff4-bfdd-8138ff72d350",
   "metadata": {},
   "source": [
    "### Generate a sample using the above prompt\n",
    "\n",
    "With suffix `x`, we generate text with the following prompt.\n",
    "\n",
    "prompt = \"Generate some news articles about politics using keywords {keywords} in around {words} words.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a6da88-8695-4cdd-979e-9e7cd940676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name  length  count_sentences  count_tokens  \\\n",
      "0  001.txt    2601               19           511   \n",
      "1  002.txt    2326               19           425   \n",
      "2  003.txt    3109               26           604   \n",
      "3  004.txt    1471               13           277   \n",
      "4  005.txt    2860               24           579   \n",
      "\n",
      "                                            keywords  \n",
      "0  pay:maternity:months:said:would:plans:six:new:...  \n",
      "1  information:said:freedom:mr:new:thomas:commiss...  \n",
      "2  women:six:hewitt:sexism:jobs:men:months:work:c...  \n",
      "3  blackpool:party:manchester:labour:conference:m...  \n",
      "4  would:mr:brown:balls:said:election:chancellor:...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[166] prompt: Generate some news articles about politics using keywords blunkett and mr and home and said and love and bbc and job and quinn and secretary and visa in around 425 words.\n",
      "[167] prompt: Generate some news articles about politics using keywords said and government and housing and homes and environmental and report and england and sustainable and communities and john in around 450 words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[168] prompt: Generate some news articles about politics using keywords murder and guilty and sentences and committee and murderers and mps and sentence and said and plea and home in around 715 words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[169] prompt: Generate some news articles about politics using keywords hunting and dogs and mr and offence and away and said and bradshaw and would and define and new in around 480 words.\n",
      "[170] prompt: Generate some news articles about politics using keywords patients and said and powys and hospital and health and hereford and welsh and board and waiting and months in around 478 words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[171] prompt: Generate some news articles about politics using keywords sports and would and said and children and schools and tories and two and hours and week and clubs in around 252 words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "proj = {\n",
    "    'output_folder': r'data\\generated',\n",
    "    'prompt_path': 'prompt_{0}.txt',\n",
    "    'output_suffix': 'x',  # one suffix for each variant associated with a specific type of prompt\n",
    "    'index_from': 165,  # -1 to start from index == 0\n",
    "}\n",
    "\n",
    "if not os.path.exists(proj['output_folder']):\n",
    "    os.makedirs(proj['output_folder'])\n",
    "        \n",
    "df = pd.read_csv(\"keywords.csv\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "def gen_prompt(keywords=['election'], words=500):\n",
    "    keywords = ' and '.join(keywords)\n",
    "    prompt = f'''Generate some news articles about politics using keywords {keywords} in around {words} words.'''  # for suffix 'x'\n",
    "    return prompt\n",
    "\n",
    "\n",
    "prompts = []\n",
    "for index, row in df.iterrows():\n",
    "    keywords = row['keywords'].split(':')  # keywords = ['election', 'politics']  # , 'riots']\n",
    "    prompt = gen_prompt(keywords=keywords, words=row['count_tokens']+np.random.randint(low=-50, high=50))\n",
    "    prompts.append(prompt)\n",
    "\n",
    "with open(proj['prompt_path'].format(proj['output_suffix']), 'a') as file:\n",
    "    for index, prompt in enumerate(prompts):\n",
    "        file.write(f\"[{index+1:03d}] prompt: {prompt}\\n\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index < proj['index_from']:\n",
    "        continue\n",
    "    prompt = prompts[index]\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.generate(**model_inputs, **generation_parameters)\n",
    "    generated_ids_without_prompt = generated_ids[0][len(model_inputs['input_ids'][0]):].unsqueeze(0)\n",
    "    output = tokenizer.batch_decode(generated_ids_without_prompt, skip_special_tokens=False)[0]\n",
    "    print(f\"[{index+1:03d}] prompt:\", prompt)\n",
    "    if index < 10:\n",
    "        print(output)\n",
    "        print(\"---\")\n",
    "    file_path = os.path.join(proj['output_folder'], f\"{row['name'][0:3]}{proj['output_suffix']}.txt\")\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(output)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513628d-46ee-4b21-9308-8e4a0d47d286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generated_ids = model.generate(**model_inputs, **generation_parameters)\n",
    "# generated_ids_without_prompt = generated_ids[0][len(model_inputs['input_ids'][0]):].unsqueeze(0)\n",
    "# tokenizer.batch_decode(generated_ids_without_prompt, skip_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d471f7-7ce6-4a4c-8f4d-e146d181beb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
